{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the pre-processing steps on the audio files and the feature extraction process:\n",
    "\n",
    "    1) Split data for training/validation (80%) and test (20%)\n",
    "    2) Data segmentation, verification, and augmentation\n",
    "    3) Feature selection to generate feature matrix and target vector\n",
    "\n",
    "# 1. Data Split for in a Train_Validation and Test_Set \n",
    "\n",
    "All the raw audio files for the 10 classes are in a `project_audio_raw` folder and the description of the audio files is located in the `file_description.csv`.  Filenames are randomly placed in 2 folders: a training/validation (80%) and a test (20%) folder.  The split is balanced on categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main working directory\n",
    "parent_wd = os.getcwd()\n",
    "source = parent_wd+'\\\\project_audio_raw\\\\'\n",
    "destination_train = parent_wd+'\\\\train_audio\\\\'\n",
    "destination_test = parent_wd+'\\\\test_audio\\\\'\n",
    "\n",
    "#get the file names and categories\n",
    "df_infos = pd.read_csv(parent_wd + '\\\\data\\\\file_description.csv')\n",
    "filenames = df_infos['filename']\n",
    "category  = df_infos['category']\n",
    "\n",
    "#Make the split on file names\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_names, test_names = train_test_split(\n",
    "    filenames, train_size=0.8, test_size=0.2, random_state=0, stratify=category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Audio data segmentation, rejection, and augmentation\n",
    "\n",
    "According the file name list, the corresponding wavefile is read with a sampling frequency of 32,000 Hz.  \n",
    "\n",
    "*  __Segmentation:__ the 5 second input wavefile is cut in 9 segments, each last 1 second with 50% overlap.  \n",
    "*  __Rejection:__ if the amplitude of the signal is smaller than 0.1 then the file is rejected.  \n",
    "*  __Augmentation:__ random noise is added to each odd file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train set: go through the filenames for data segmentation, augmentation and rejection\n",
    "#train_target_vector = np.empty(0)\n",
    "## extract the file\n",
    "#for train_name in train_names:\n",
    "#    os.chdir(source)\n",
    "#    y, sr = librosa.load(train_name, sr = 32000)\n",
    "#    name = train_name.split('\\\\')[-1].split('.')[0]\n",
    "#    os.chdir(destination_train)\n",
    "#    for i in np.arange(5):\n",
    "#        extract = y[sr*i:sr*(i+1)]\n",
    "#        vol_rms =  y.max() - y.min()\n",
    "#        # remove the files if no signal is in\n",
    "#        if vol_rms > 0.05: \n",
    "#            extract_name = name + '_C' + str(i) + '.wav' #C stands for clean\n",
    "#            sf.write(extract_name, extract, sr)\n",
    "#            train_target_vector = np.append(train_target_vector, [extract_name], axis=0)\n",
    "#        else:\n",
    "#            pass\n",
    "#    #data augmentation with random noise\n",
    "#    for j in np.arange(4):\n",
    "#        extract_2 = y[int(sr/2)+j*sr  : int(sr/2) + sr*(j+1)]\n",
    "#        vol_rms =  y.max() - y.min()\n",
    "#        if vol_rms > 0.1:\n",
    "#            noise = np.random.normal(0,0.01,len(extract_2)) # add noise\n",
    "#            extract_2_name = name + '_N' + str(j) + '.wav' #N stands for noise\n",
    "#            sf.write(extract_2_name, extract_2, sr)\n",
    "#            train_target_vector = np.append(train_target_vector, [extract_2_name], axis=0)\n",
    "#        else:\n",
    "#            pass\n",
    "#\n",
    "#os.chdir(parent_wd+'\\\\data')\n",
    "#np.save('files_train.npy', train_target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test set: go through the filenames for data segmentation, augmentation and rejection\n",
    "#test_target_vector = np.empty(0)\n",
    "## extract the file\n",
    "#for test_name in test_names:\n",
    "#    os.chdir(source)\n",
    "#    y, sr = librosa.load(test_name, sr = 32000)\n",
    "#    name = test_name.split('\\\\')[-1].split('.')[0]\n",
    "#    os.chdir(destination_test)\n",
    "#    for i in np.arange(5):\n",
    "#        extract = y[sr*i:sr*(i+1)]\n",
    "#        vol_rms =  y.max() - y.min()\n",
    "#        # remove the files if no signal is in\n",
    "#        if vol_rms > 0.05: \n",
    "#            extract_name = name + '_C' + str(i) + '.wav' #C stands for clean\n",
    "#            sf.write(extract_name, extract, sr)\n",
    "#            test_target_vector = np.append(test_target_vector, [extract_name], axis=0)\n",
    "#        else:\n",
    "#            pass\n",
    "#    #data augmentation with random noise\n",
    "#    for j in np.arange(4):\n",
    "#        extract_2 = y[int(sr/2)+j*sr  : int(sr/2) + sr*(j+1)]\n",
    "#        vol_rms =  y.max() - y.min()\n",
    "#        if vol_rms > 0.1:\n",
    "#            noise = np.random.normal(0,0.01,len(extract_2)) # add noise\n",
    "#            extract_2_name = name + '_N' + str(j) + '.wav' #N stands for noise\n",
    "#            sf.write(extract_2_name, extract_2, sr)\n",
    "#            test_target_vector = np.append(test_target_vector, [extract_2_name], axis=0)\n",
    "#        else:\n",
    "#            pass\n",
    "#\n",
    "#os.chdir(parent_wd+'\\\\data')\n",
    "#np.save('files_test.npy', test_target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Features extraction\n",
    "\n",
    "## 3.1 MFCCs setting 1: 63 x 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row length:  3969\n",
      "number of frames:  63\n"
     ]
    }
   ],
   "source": [
    "sampling_f = 32000 # sampling frequency in Hz\n",
    "sample_length = 1 # sample duration in seconds\n",
    "hop_length = 512 # number of samples between 2 analysis points\n",
    "n_frames = 63# number of frames for each sample\n",
    "n_mfcc = 63 # number of coefficients for the mfcc analysis \n",
    "row_length = n_mfcc*n_frames #define the row length\n",
    "print('row length: ', row_length)\n",
    "print('number of frames: ', n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file with file name and class\n",
    "meta_data = np.load(parent_wd+'\\\\data\\\\files_train.npy')\n",
    "\n",
    "# create a list with all the wav files in the 'project_audio' folder \n",
    "os.chdir(parent_wd + '\\\\train_audio')\n",
    "filenames = glob.glob('*.wav')\n",
    "\n",
    "#initiate features and target\n",
    "feature_matrix, labels = np.empty((0, n_mfcc*n_frames)), np.empty(0)\n",
    "\n",
    "#Extract features\n",
    "for filename in filenames:\n",
    "    raw_sound, sample_rate = librosa.load(filename, sr=sampling_f)\n",
    "    #get the MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=raw_sound, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    mfccs_flat = mfccs.reshape(1,row_length)    \n",
    "    mfcc_flat_scl = sklearn.preprocessing.scale(mfccs_flat, axis=1) # scale the flattend features\n",
    "    feature_matrix = np.append(feature_matrix, mfcc_flat_scl, axis=0)\n",
    "    #get the label\n",
    "    row_label = filename.split('_')[0].split('-')[-1]\n",
    "    labels = np.append(labels, [row_label], axis=0)\n",
    "\n",
    "#define the dictionary based on the documentation\n",
    "dictionary = {'14':0, '13': 1, '10': 2, '11': 3, '16':4, '43': 5, '46':6, '44':7, '42':8, '45':9}\n",
    "#change the labels according to the dictionary\n",
    "y = pd.Series(labels).replace(dictionary)\n",
    "\n",
    "#save the data\n",
    "os.chdir(parent_wd+'\\\\data')\n",
    "np.save('X_63_train.npy', feature_matrix)\n",
    "np.save('y_63_train.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file with file name and class\n",
    "meta_data = np.load(parent_wd+'\\\\data\\\\files_test.npy')\n",
    "# create a list with all the wav files in the 'project_audio' folder \n",
    "os.chdir(parent_wd + '\\\\test_audio')\n",
    "filenames = glob.glob('*.wav')\n",
    "\n",
    "#initiate features and target\n",
    "feature_matrix, labels = np.empty((0, n_mfcc*n_frames)), np.empty(0)\n",
    "\n",
    "#Extract features\n",
    "for filename in filenames:\n",
    "    raw_sound, sample_rate = librosa.load(filename, sr=sampling_f)\n",
    "    #get the MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=raw_sound, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    mfccs_flat = mfccs.reshape(1,row_length)    \n",
    "    mfcc_flat_scl = sklearn.preprocessing.scale(mfccs_flat, axis=1) # scale the flattend features\n",
    "    feature_matrix = np.append(feature_matrix, mfcc_flat_scl, axis=0)\n",
    "    #get the label\n",
    "    row_label = filename.split('_')[0].split('-')[-1]\n",
    "    labels = np.append(labels, [row_label], axis=0)\n",
    "\n",
    "#define the dictionary based on the documentation\n",
    "dictionary = {'14':0, '13': 1, '10': 2, '11': 3, '16':4, '43': 5, '46':6, '44':7, '42':8, '45':9}\n",
    "#change the labels according to the dictionary\n",
    "y = pd.Series(labels).replace(dictionary)\n",
    "\n",
    "#save the data\n",
    "os.chdir(parent_wd+'\\\\data')\n",
    "np.save('X_63_test.npy', feature_matrix)\n",
    "np.save('y_63_test.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MFCCs setting 2: 32 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row length:  1024\n",
      "number of frames:  32\n"
     ]
    }
   ],
   "source": [
    "sampling_f = 32000 # sampling frequency in Hz\n",
    "sample_length = 1 # sample duration in seconds\n",
    "hop_length = 1024 # number of samples between 2 analysis points\n",
    "n_frames = 32# number of frames for each sample\n",
    "n_mfcc = 32 # number of coefficients for the mfcc analysis \n",
    "row_length = n_mfcc*n_frames #define the row length\n",
    "print('row length: ', row_length)\n",
    "print('number of frames: ', n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file with file name and class\n",
    "meta_data = np.load(parent_wd+'\\\\data\\\\files_train.npy')\n",
    "\n",
    "# create a list with all the wav files in the 'project_audio' folder \n",
    "os.chdir(parent_wd + '\\\\train_audio')\n",
    "filenames = glob.glob('*.wav')\n",
    "\n",
    "#initiate features and target\n",
    "feature_matrix, labels = np.empty((0, n_mfcc*n_frames)), np.empty(0)\n",
    "\n",
    "#Extract features\n",
    "for filename in filenames:\n",
    "    raw_sound, sample_rate = librosa.load(filename, sr=sampling_f)\n",
    "    #get the MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=raw_sound, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    mfccs_flat = mfccs.reshape(1,row_length)    \n",
    "    mfcc_flat_scl = sklearn.preprocessing.scale(mfccs_flat, axis=1) # scale the flattend features\n",
    "    feature_matrix = np.append(feature_matrix, mfcc_flat_scl, axis=0)\n",
    "    #get the label\n",
    "    row_label = filename.split('_')[0].split('-')[-1]\n",
    "    labels = np.append(labels, [row_label], axis=0)\n",
    "\n",
    "#define the dictionary based on the documentation\n",
    "dictionary = {'14':0, '13': 1, '10': 2, '11': 3, '16':4, '43': 5, '46':6, '44':7, '42':8, '45':9}\n",
    "#change the labels according to the dictionary\n",
    "y = pd.Series(labels).replace(dictionary)\n",
    "\n",
    "#save the data\n",
    "os.chdir(parent_wd+'\\\\data')\n",
    "np.save('X_32_train.npy', feature_matrix)\n",
    "np.save('y_32_train.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file with file name and class\n",
    "meta_data = np.load(parent_wd+'\\\\data\\\\files_test.npy')\n",
    "# create a list with all the wav files in the 'project_audio' folder \n",
    "os.chdir(parent_wd + '\\\\test_audio')\n",
    "filenames = glob.glob('*.wav')\n",
    "\n",
    "#initiate features and target\n",
    "feature_matrix, labels = np.empty((0, n_mfcc*n_frames)), np.empty(0)\n",
    "\n",
    "#Extract features\n",
    "for filename in filenames:\n",
    "    raw_sound, sample_rate = librosa.load(filename, sr=sampling_f)\n",
    "    #get the MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=raw_sound, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    mfccs_flat = mfccs.reshape(1,row_length)    \n",
    "    mfcc_flat_scl = sklearn.preprocessing.scale(mfccs_flat, axis=1) # scale the flattend features\n",
    "    feature_matrix = np.append(feature_matrix, mfcc_flat_scl, axis=0)\n",
    "    #get the label\n",
    "    row_label = filename.split('_')[0].split('-')[-1]\n",
    "    labels = np.append(labels, [row_label], axis=0)\n",
    "\n",
    "#define the dictionary based on the documentation\n",
    "dictionary = {'14':0, '13': 1, '10': 2, '11': 3, '16':4, '43': 5, '46':6, '44':7, '42':8, '45':9}\n",
    "#change the labels according to the dictionary\n",
    "y = pd.Series(labels).replace(dictionary)\n",
    "\n",
    "#save the data\n",
    "os.chdir(parent_wd+'\\\\data')\n",
    "np.save('X_32_test.npy', feature_matrix)\n",
    "np.save('y_32_test.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mel-spectrogram and MFCC 3 channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row length:  1024\n",
      "number of frames:  32\n"
     ]
    }
   ],
   "source": [
    "sampling_f = 32000 # sampling frequency in Hz\n",
    "sample_length = 1 # sample duration in seconds\n",
    "hop_length = 1024 # number of samples between 2 analysis points\n",
    "n_frames = 32# number of frames for each sample\n",
    "n_mfcc = 32 # number of coefficients for the mfcc analysis \n",
    "row_length = n_mfcc*n_frames #define the row length\n",
    "print('row length: ', row_length)\n",
    "print('number of frames: ', n_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file with file name and class\n",
    "meta_data = np.load(parent_wd+'\\\\data\\\\files_train.npy')\n",
    "\n",
    "\n",
    "# create a list with all the wav files in the 'project_audio' folder \n",
    "os.chdir(parent_wd + '\\\\train_audio')\n",
    "filenames = glob.glob('*.wav')\n",
    "\n",
    "#initiate features and target\n",
    "feature_matrix, labels = np.empty((0, row_length, 3)), np.empty(0)\n",
    "\n",
    "#Extract features\n",
    "for filename in filenames:\n",
    "    raw_sound, sample_rate = librosa.load(filename, sr=sampling_f)\n",
    "    \n",
    "    ## feature 1 MELSPEC\n",
    "    melspec = librosa.feature.melspectrogram(y=raw_sound, n_mels=32, hop_length=hop_length)\n",
    "    melspec_flat = melspec.reshape(1, row_length)\n",
    "    melspec_flat_scl = sklearn.preprocessing.scale(melspec_flat, axis=1)\n",
    "    \n",
    "    ## feature 2 MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=raw_sound, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    mfccs_flat = mfccs.reshape(1,row_length)    \n",
    "    mfcc_flat_scl = sklearn.preprocessing.scale(mfccs_flat, axis=1)\n",
    "    \n",
    "    ## feature 3 delta MFCC\n",
    "    mfcc_delta = librosa.feature.delta(mfccs, order=1, axis=1)\n",
    "    mfcc_delta_flat = mfcc_delta.reshape(1, row_length)\n",
    "    mfcc_delta_flat_scl = sklearn.preprocessing.scale(mfcc_delta_flat, axis=1)\n",
    "    \n",
    "    #bind in 3d array\n",
    "    bind = np.array([melspec_flat_scl, mfcc_flat_scl, mfcc_delta_flat_scl])\n",
    "    bind_t = np.transpose(bind, axes=[1,2,0])\n",
    "    feature_matrix = np.concatenate((feature_matrix, bind_t),axis=0)\n",
    "    \n",
    "    #get the label\n",
    "    row_label = filename.split('_')[0].split('-')[-1]\n",
    "    labels = np.append(labels, [row_label], axis=0)\n",
    "\n",
    "#define the dictionary based on the documentation\n",
    "dictionary = {'14':0, '13': 1, '10': 2, '11': 3, '16':4, '43': 5, '46':6, '44':7, '42':8, '45':9}\n",
    "#change the labels according to the dictionary\n",
    "y = pd.Series(labels).replace(dictionary)\n",
    "\n",
    "#save the data\n",
    "os.chdir(parent_wd+'\\\\data')\n",
    "np.save('X_3d_train.npy', feature_matrix)\n",
    "np.save('y_3d_train.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file with file name and class\n",
    "meta_data = np.load(parent_wd+'\\\\data\\\\files_test.npy')\n",
    "\n",
    "# create a list with all the wav files in the 'project_audio' folder \n",
    "os.chdir(parent_wd + '\\\\test_audio')\n",
    "filenames = glob.glob('*.wav')\n",
    "\n",
    "#initiate features and target\n",
    "feature_matrix, labels = np.empty((0, row_length, 3)), np.empty(0)\n",
    "\n",
    "#Extract features\n",
    "for filename in filenames:\n",
    "    raw_sound, sample_rate = librosa.load(filename, sr=sampling_f)\n",
    "    \n",
    "    ## feature 1 MELSPEC\n",
    "    melspec = librosa.feature.melspectrogram(y=raw_sound, n_mels=32, hop_length=hop_length)\n",
    "    melspec_flat = melspec.reshape(1, row_length)\n",
    "    melspec_flat_scl = sklearn.preprocessing.scale(melspec_flat, axis=1)\n",
    "    \n",
    "    ## feature 2 MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=raw_sound, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    mfccs_flat = mfccs.reshape(1,row_length)    \n",
    "    mfcc_flat_scl = sklearn.preprocessing.scale(mfccs_flat, axis=1)\n",
    "    \n",
    "    ## feature 3 delta MFCC\n",
    "    mfcc_delta = librosa.feature.delta(mfccs, order=1, axis=1)\n",
    "    mfcc_delta_flat = mfcc_delta.reshape(1, row_length)\n",
    "    mfcc_delta_flat_scl = sklearn.preprocessing.scale(mfcc_delta_flat, axis=1)\n",
    "    \n",
    "    #bind in 3d array\n",
    "    bind = np.array([melspec_flat_scl, mfcc_flat_scl, mfcc_delta_flat_scl])\n",
    "    bind_t = np.transpose(bind, axes=[1,2,0])\n",
    "    feature_matrix = np.concatenate((feature_matrix, bind_t),axis=0)\n",
    "    \n",
    "    #get the label\n",
    "    row_label = filename.split('_')[0].split('-')[-1]\n",
    "    labels = np.append(labels, [row_label], axis=0)\n",
    "\n",
    "#define the dictionary based on the documentation\n",
    "dictionary = {'14':0, '13': 1, '10': 2, '11': 3, '16':4, '43': 5, '46':6, '44':7, '42':8, '45':9}\n",
    "#change the labels according to the dictionary\n",
    "y = pd.Series(labels).replace(dictionary)\n",
    "\n",
    "#save the data\n",
    "os.chdir(parent_wd+'\\\\data')\n",
    "np.save('X_3d_test.npy', feature_matrix)\n",
    "np.save('y_3d_test.npy', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
